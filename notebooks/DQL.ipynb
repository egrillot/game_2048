{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepQlearner Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game_2048.env import env_2048\n",
    "from game_2048.agent import DeepQlearner, espilon_greedy_search\n",
    "from game_2048.utils import exponential_espilon_decrease\n",
    "\n",
    "env = env_2048()\n",
    "agent = DeepQlearner('deepQlearner agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent deepQlearner agent\n",
      "with parameters : DQL: trainable parameters=522500,            lr=0.01, target_update=10, Î³=0.015, memory_length=10000,            batch_size=512\n",
      "\n",
      "Epsilon greedy search parameters : max_iteration=1000, epochs=5000 and epsilon=0.99.\n",
      "\n",
      "Epoch : 1/5000, epsilon = 0.99\n",
      "1000/1000 [==============================] - 0s 44us/iteration - cumulate rewards: 55896.0 - max reached: 64 - model loss: None             \n",
      "\n",
      "Epoch ended at 111 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 2/5000, epsilon = 0.9898803493048243\n",
      "1000/1000 [==============================] - 0s 32us/iteration - cumulate rewards: 36346.0 - max reached: 64 - model loss: None             \n",
      "\n",
      "Epoch ended at 91 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 3/5000, epsilon = 0.9896061191753511\n",
      "1000/1000 [==============================] - 0s 36us/iteration - cumulate rewards: 42214.0 - max reached: 64 - model loss: None             \n",
      "\n",
      "Epoch ended at 100 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 4/5000, epsilon = 0.9891213341405545\n",
      "1000/1000 [==============================] - 0s 23us/iteration - cumulate rewards: 18358.0 - max reached: 64 - model loss: None             \n",
      "\n",
      "Epoch ended at 69 iterations - Exploit deflected to explore count : 0 - Exploit count : 0. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 5/5000, epsilon = 0.9886773445638092\n",
      "1000/1000 [==============================] - 0s 24us/iteration - cumulate rewards: 15780.0 - max reached: 64 - model loss: None             \n",
      "\n",
      "Epoch ended at 65 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 6/5000, epsilon = 0.9881770160517893\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 96250.0 - max reached: 128 - model loss: 152.8126220703125                   \n",
      "\n",
      "Epoch ended at 141 iterations - Exploit deflected to explore count : 0 - Exploit count : 0. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 7/5000, epsilon = 0.9868044238084013\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 28624.0 - max reached: 64 - model loss: 141.88157653808594                  \n",
      "\n",
      "Epoch ended at 86 iterations - Exploit deflected to explore count : 0 - Exploit count : 2. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 8/5000, epsilon = 0.9857853672889507\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 31094.0 - max reached: 64 - model loss: 115.29164123535156                  \n",
      "\n",
      "Epoch ended at 85 iterations - Exploit deflected to explore count : 0 - Exploit count : 3. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 9/5000, epsilon = 0.9846409742686295\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 93692.0 - max reached: 128 - model loss: 133.22531127929688                  \n",
      "\n",
      "Epoch ended at 142 iterations - Exploit deflected to explore count : 1 - Exploit count : 2. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 10/5000, epsilon = 0.982414497836181\n",
      "1000/1000 [==============================] - 8s 8ms/iteration - cumulate rewards: 169314.0 - max reached: 256 - model loss: 141.8964385986328                   \n",
      "\n",
      "Epoch ended at 198 iterations - Exploit deflected to explore count : 0 - Exploit count : 6. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 11/5000, epsilon = 0.9786670504548977\n",
      "1000/1000 [==============================] - 4s 4ms/iteration - cumulate rewards: 59024.0 - max reached: 128 - model loss: 128.65164184570312                  \n",
      "\n",
      "Epoch ended at 110 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 12/5000, epsilon = 0.976277453255982\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 71108.0 - max reached: 128 - model loss: 103.47833251953125                  \n",
      "\n",
      "Epoch ended at 128 iterations - Exploit deflected to explore count : 0 - Exploit count : 6. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 13/5000, epsilon = 0.9732119409704291\n",
      "1000/1000 [==============================] - 2s 2ms/iteration - cumulate rewards: 6190.0 - max reached: 32 - model loss: 90.85643005371094                    \n",
      "\n",
      "Epoch ended at 44 iterations - Exploit deflected to explore count : 0 - Exploit count : 2. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 14/5000, epsilon = 0.9721058845077246\n",
      "1000/1000 [==============================] - 4s 4ms/iteration - cumulate rewards: 69956.0 - max reached: 128 - model loss: 108.6375732421875                   \n",
      "\n",
      "Epoch ended at 118 iterations - Exploit deflected to explore count : 0 - Exploit count : 3. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 15/5000, epsilon = 0.9689232799821176\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 80470.0 - max reached: 128 - model loss: 130.79425048828125                  \n",
      "\n",
      "Epoch ended at 123 iterations - Exploit deflected to explore count : 0 - Exploit count : 2. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 16/5000, epsilon = 0.9653372807259064\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 35156.0 - max reached: 64 - model loss: 114.72185516357422                  \n",
      "\n",
      "Epoch ended at 84 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 17/5000, epsilon = 0.9627431706273466\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 91834.0 - max reached: 128 - model loss: 100.54075622558594                  \n",
      "\n",
      "Epoch ended at 125 iterations - Exploit deflected to explore count : 0 - Exploit count : 7. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 18/5000, epsilon = 0.9586372221068299\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 30582.0 - max reached: 64 - model loss: 110.13361358642578                  \n",
      "\n",
      "Epoch ended at 81 iterations - Exploit deflected to explore count : 0 - Exploit count : 3. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 19/5000, epsilon = 0.9558432443405793\n",
      "1000/1000 [==============================] - 8s 8ms/iteration - cumulate rewards: 269396.0 - max reached: 256 - model loss: 167.38389587402344                  \n",
      "\n",
      "Epoch ended at 221 iterations - Exploit deflected to explore count : 1 - Exploit count : 11. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 20/5000, epsilon = 0.947583154143874\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 35816.0 - max reached: 64 - model loss: 147.5052947998047                   \n",
      "\n",
      "Epoch ended at 90 iterations - Exploit deflected to explore count : 0 - Exploit count : 1. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 21/5000, epsilon = 0.9440052056035209\n",
      "1000/1000 [==============================] - 3s 3ms/iteration - cumulate rewards: 25490.0 - max reached: 64 - model loss: 136.2330322265625                   \n",
      "\n",
      "Epoch ended at 76 iterations - Exploit deflected to explore count : 0 - Exploit count : 7. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 22/5000, epsilon = 0.9408861776732304\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 84100.0 - max reached: 128 - model loss: 98.56816101074219                   \n",
      "\n",
      "Epoch ended at 129 iterations - Exploit deflected to explore count : 0 - Exploit count : 8. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 23/5000, epsilon = 0.9353466015111064\n",
      "1000/1000 [==============================] - 7s 7ms/iteration - cumulate rewards: 170402.0 - max reached: 128 - model loss: 83.13770294189453                   \n",
      "\n",
      "Epoch ended at 183 iterations - Exploit deflected to explore count : 4 - Exploit count : 12. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 24/5000, epsilon = 0.9270092061834886\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 82338.0 - max reached: 128 - model loss: 100.74282836914062                  \n",
      "\n",
      "Epoch ended at 134 iterations - Exploit deflected to explore count : 1 - Exploit count : 12. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 25/5000, epsilon = 0.9205824699931776\n",
      "1000/1000 [==============================] - 5s 5ms/iteration - cumulate rewards: 73934.0 - max reached: 128 - model loss: 108.89280700683594                  \n",
      "\n",
      "Epoch ended at 120 iterations - Exploit deflected to explore count : 1 - Exploit count : 11. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 26/5000, epsilon = 0.9145991086264821\n",
      "1000/1000 [==============================] - 4s 4ms/iteration - cumulate rewards: 43314.0 - max reached: 64 - model loss: 112.99925231933594                  \n",
      "\n",
      "Epoch ended at 97 iterations - Exploit deflected to explore count : 1 - Exploit count : 5. \n",
      "\n",
      "###################\n",
      "\n",
      "Epoch : 27/5000, epsilon = 0.9096151598086126\n",
      "  14/1000 [>..............................] - estimated time remaining: 40s - cumulate rewards: 1476.0 - max reached: 16 - model loss: 138.74404907226562     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m espilon_greedy_search(\n\u001b[0;32m      2\u001b[0m     agent\u001b[39m=\u001b[39;49magent,\n\u001b[0;32m      3\u001b[0m     env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m      4\u001b[0m     max_iteration\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m,\n\u001b[0;32m      7\u001b[0m     decrease_function\u001b[39m=\u001b[39;49mexponential_espilon_decrease(epsilon_min\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, exponential_decay\u001b[39m=\u001b[39;49m\u001b[39m50000000\u001b[39;49m),\n\u001b[0;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\Documents\\projects\\game_2048\\game_2048\\agent\\training.py:97\u001b[0m, in \u001b[0;36mespilon_greedy_search\u001b[1;34m(agent, max_iteration, epochs, epsilon, decrease_function, env, verbose)\u001b[0m\n\u001b[0;32m     93\u001b[0m p \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom()\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m epsilon:\n\u001b[1;32m---> 97\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mexplore(env)\n\u001b[0;32m     98\u001b[0m     action_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexplore\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    100\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\projects\\game_2048\\game_2048\\agent\\dql.py:190\u001b[0m, in \u001b[0;36mDeepQlearner.explore\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexplore\u001b[39m(\u001b[39mself\u001b[39m, env: env_2048) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39;49msample_valid_action()\n",
      "File \u001b[1;32m~\\Documents\\projects\\game_2048\\game_2048\\env\\env.py:157\u001b[0m, in \u001b[0;36menv_2048.sample_valid_action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m action_to_test \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[0;32m    155\u001b[0m action_to_test\u001b[39m.\u001b[39mremove(action)\n\u001b[1;32m--> 157\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_action_valid(action):\n\u001b[0;32m    158\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(action_to_test)\n\u001b[0;32m    159\u001b[0m     action_to_test\u001b[39m.\u001b[39mremove(action)\n",
      "File \u001b[1;32m~\\Documents\\projects\\game_2048\\game_2048\\env\\env.py:143\u001b[0m, in \u001b[0;36menv_2048.is_action_valid\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_up_action()\n\u001b[0;32m    142\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_right_action()\n\u001b[0;32m    144\u001b[0m \u001b[39melif\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_down_action()\n",
      "File \u001b[1;32m~\\Documents\\projects\\game_2048\\game_2048\\env\\env.py:77\u001b[0m, in \u001b[0;36menv_2048.on_right_action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m row[j]\n\u001b[0;32m     75\u001b[0m         row[:j] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(row[:j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], pad_width\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m, constant_values\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m---> 77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[i, :] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mpad(row, pad_width\u001b[39m=\u001b[39;49m(\u001b[39m4\u001b[39;49m \u001b[39m-\u001b[39;49m row\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m0\u001b[39;49m), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m'\u001b[39;49m, constant_values\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\emili\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\game-2048-G23fBW8X-py3.10\\lib\\site-packages\\numpy\\lib\\arraypad.py:794\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m stat_functions \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmaximum\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mamax, \u001b[39m\"\u001b[39m\u001b[39mminimum\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mamin,\n\u001b[0;32m    790\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmean, \u001b[39m\"\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmedian}\n\u001b[0;32m    792\u001b[0m \u001b[39m# Create array with final shape and original values\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[39m# (padded area is undefined)\u001b[39;00m\n\u001b[1;32m--> 794\u001b[0m padded, original_area_slice \u001b[39m=\u001b[39m _pad_simple(array, pad_width)\n\u001b[0;32m    795\u001b[0m \u001b[39m# And prepare iteration over all dimensions\u001b[39;00m\n\u001b[0;32m    796\u001b[0m \u001b[39m# (zipping may be more readable than using enumerate)\u001b[39;00m\n\u001b[0;32m    797\u001b[0m axes \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(padded\u001b[39m.\u001b[39mndim)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "espilon_greedy_search(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    max_iteration=1000,\n",
    "    epochs=5000,\n",
    "    epsilon=0.99,\n",
    "    decrease_function=exponential_espilon_decrease(epsilon_min=0.01, exponential_decay=50000000),\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-2048-ivi8wBEA-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
